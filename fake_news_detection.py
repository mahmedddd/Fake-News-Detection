# -*- coding: utf-8 -*-
"""Fake_News_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aJw67f3QTYCMP6Ts3oc5vLrHVO5yU92E

#  Part 1: Uploading Dataset to GitHub Repository

### Navigate to cloned GitHub repo
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Fake-News-Detection/

"""###Cloned GitHub repository with token"""

# Commented out IPython magic to ensure Python compatibility.
from getpass import getpass
username = "mahmedddd"
repo_name = "Fake-News-Detection"
token = getpass("Enter your GitHub token: ")
repo_url = f"https://{token}@github.com/{username}/{repo_name}.git"

# %cd /content/
!git clone $repo_url

"""### Move dataset into repoâ€™s data folder"""

import os

repo_name = "Fake-News-Detection"
repo_path = f"/content/{repo_name}"
os.makedirs(f"{repo_path}/data", exist_ok=True)
!mv /content/fake-and-real-news-dataset.zip {repo_path}/data/

# README.md
readme_content = '''# Fake News Detection

This repository contains code and data for detecting fake news.

## ðŸ“¥ Dataset

Download manually from [Google Drive](https://drive.google.com/file/d/1ysMZgKPEqHOX6QQyXRf9AYAENqsLHOb9/view?usp=sharing)

Or use this code to download automatically:

```python
!pip install -q gdown
!gdown --id 1ysMZgKPEqHOX6QQyXRf9AYAENqsLHOb9 -O fake-and-real-news-dataset.zip
!unzip -q fake-and-real-news-dataset.zip

"""### Add, commit, and push all repo changes"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Fake-News-Detection
!git config --global user.email "ahmedunited902@gmail.com"
!git config --global user.name "mahmedddd"

!git add .
!git commit -m "Add dataset and README with download instructions"
!git push origin main

"""# Part 2: Fake News Detection â€” Preprocessing, Modeling, Evaluation

### Unzip the dataset into the /data folder
"""

import zipfile
import os

# Path to the zip file
zip_path = "/content/Fake-News-Detection/data/fake-and-real-news-dataset.zip"
extract_dir = "/content/Fake-News-Detection/data"

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List contents to confirm extraction
print("Extracted files:")
print(os.listdir(extract_dir))

"""###  Load and label fake & true datasets"""

import pandas as pd

# Load fake and real news datasets
df_fake = pd.read_csv("/content/Fake-News-Detection/data/Fake.csv")
df_true = pd.read_csv("/content/Fake-News-Detection/data/True.csv")

# Assign labels: 0 = fake, 1 = true
df_fake["label"] = 0
df_true["label"] = 1

# Combine the datasets
df = pd.concat([df_fake, df_true], ignore_index=True)

# Keep only relevant columns
df = df[["title", "text", "label"]]

# Optional: Shuffle the dataset to mix fake and real examples
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Preview the combined dataset
df.head()

"""### Preprocessing - clean, remove stopwords, lemmatize"""

import nltk
import spacy
import re
from nltk.corpus import stopwords

# Download NLTK stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Load spaCy English model
import en_core_web_sm
nlp = en_core_web_sm.load()

# Function to clean, lemmatize, and remove stopwords
def preprocess(text):
    # Combine title and body text
    text = str(text)
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = text.lower()  # Lowercase

    doc = nlp(text)
    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.is_alpha]
    return ' '.join(tokens)

# Apply to both title and text combined
df["combined_text"] = df["title"].fillna('') + " " + df["text"].fillna('')
df["clean_text"] = df["combined_text"].apply(preprocess)

df[["clean_text", "label"]].head()

"""### Vectorization with TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize vectorizer
vectorizer = TfidfVectorizer(max_features=5000)  # You can tune max_features

# Fit and transform
X = vectorizer.fit_transform(df["clean_text"])
y = df["label"]

"""### Train/Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""### Training Classifier - SVM"""

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Train model
svm_model = LinearSVC()
svm_model.fit(X_train, y_train)

# Predict
y_pred_svm = svm_model.predict(X_test)

# Evaluate
print("SVM Results:")
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("F1 Score:", f1_score(y_test, y_pred_svm))
print("\nClassification Report:\n", classification_report(y_test, y_pred_svm))

"""## **BONUS:**"""

!pip install -q wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Separate fake and real news texts
fake_text = " ".join(df[df["label"] == 0]["clean_text"].tolist())
real_text = " ".join(df[df["label"] == 1]["clean_text"].tolist())

# Generate word clouds
fake_wc = WordCloud(width=800, height=400, background_color='white').generate(fake_text)
real_wc = WordCloud(width=800, height=400, background_color='white').generate(real_text)

# Plot side by side
plt.figure(figsize=(16, 8))

plt.subplot(1, 2, 1)
plt.imshow(fake_wc, interpolation='bilinear')
plt.axis('off')
plt.title("Fake News Word Cloud", fontsize=16)

plt.subplot(1, 2, 2)
plt.imshow(real_wc, interpolation='bilinear')
plt.axis('off')
plt.title("Real News Word Cloud", fontsize=16)

plt.tight_layout()
plt.show()

